# 任务一

写在前面：  
由于我是大二登，之前也自学过机器学习，所以学习部分略过，直接回答问题，还请理解。

---

## 1. 什么是机器学习

定义  
机器学习是利用数据和算法，使模型在完成特定任务时的性能随着经验（数据）不断提升的过程。

解释与示例  
- 序列预测：学习已有序列的通项公式，预测下一个元素  
- 分类问题：将带标签的样本分到不同类别中（如垃圾邮件识别）  
- 聚类问题：在无标签数据中发现内在结构（如客户分群）  

补充  
经验可以来自历史观测、用户行为或传感器采集的数据，不同算法依赖不同形式的“经验”来更新模型。

---

## 2. 深度学习中的“深度”及主要优势

定义  
“深度”指的是神经网络中可训练层的数量，也就是网络的层级结构深度。

主要优势  
- 自动特征提取：多层网络能在低层捕捉简单特征，在高层组合成复杂特征  
- 端到端训练：从原始输入到最终输出，整个网络一起优化，无需手工设计特征  
- 在大规模数据和海量参数下表现优于传统方法  

示例  
卷积神经网络（CNN）在图像处理中通过多层卷积和池化自动提取从边缘到物体的层次化特征。

---

## 3. 监督学习 vs. 无监督学习

核心区别  
- 监督学习：训练集包含输入–标签对，目标是学习映射 \(x \mapsto y\)  
- 无监督学习：训练集只有输入，不依赖标签，目标是发现数据结构和分布  

典型任务  
- 监督学习：猫狗图像分类  
- 无监督学习：K-means 聚类将用户分群  

---

## 4. 过拟合（Overfitting）

定义  
模型在训练数据上表现极好，在未见过的数据上表现显著下降的现象。

常见原因  
- 模型容量过大（参数过多）  
- 训练样本太少或噪声过多  
- 训练迭代次数过多  

防止/减轻方法  
- 增加训练数据或数据增强  
- 降低模型复杂度  
- 使用 L1/L2 正则化、Dropout 或早停（Early Stopping）  
- 利用预训练模型并进行微调  

---

## 5. 欠拟合（Underfitting）

定义  
模型过于简单，无法捕捉训练数据中的规律，训练和测试误差均较高。

改善策略  
- 增加模型深度或宽度  
- 使用非线性激活函数（从线性模型切换到 MLP）  
- 降低正则化强度  
- 添加更多特征或进行特征工程  

---

## 6. 偏差（Bias）与方差（Variance）及权衡

定义  
- 偏差：模型预测值与真实值的平均差异，用于衡量学习算法的拟合程度  
- 方差：模型对不同训练集变化的敏感度，即预测结果的波动幅度  

与拟合的关系  
- 高偏差/低方差：欠拟合  
- 低偏差/高方差：过拟合  

偏差-方差权衡  
不能同时最小化偏差和方差，需要在模型复杂度与泛化能力之间找到平衡点。

---

## 7. 损失函数与常见类型

定义  
损失函数用于量化模型输出与真实值之间的差距，是训练过程中需要最小化的目标。

常见形式  
- 均方误差（MSE）：  
  \[
  \mathrm{MSE} = \frac{1}{N}\sum_{i=1}^N (y_i - \hat y_i)^2
  \]  
  适用回归任务  
- 交叉熵损失（Cross-Entropy）：  
  \[
  L = -\frac{1}{N}\sum_{i=1}^N \sum_{c=1}^C y_{i,c}\log \hat y_{i,c}
  \]  
  适用分类任务  

---

## 8. 神经网络的隐藏层

定义  
隐藏层位于输入层与输出层之间，不直接与外部数据交互，用于组合与变换特征。

作用  
- 引入非线性：在隐藏层后加入激活函数  
- 提取层次化特征：深层隐藏层捕获更抽象的模式  
- 增强表达能力：多层网络能近似任意复杂函数  

---

## 9. 激活函数

定义  
激活函数决定神经元是否以及以何种强度向下一层传递信号，相当于“阈值判断”。

经典类比  
当皮肤被螨虫轻轻一爬时，不会触发痛感；当力道超过阈值时才产生反馈。这个“阈值判断”过程就像阶跃函数。  

常见激活函数  
- 阶跃函数（Step Function）  
- Sigmoid：\(\sigma(x)=1/(1+e^{-x})\)  
- Tanh：\(\tanh(x)=(e^{x}-e^{-x})/(e^{x}+e^{-x})\)  
- ReLU：\(\max(0, x)\)  

---

## 10. 梯度下降（Gradient Descent）

定义  
一种一阶优化算法，通过迭代沿负梯度方向更新参数来最小化目标函数。

更新规则  
\[
\theta \leftarrow \theta - \eta\,\nabla_\theta J(\theta)
\]  
- \(\theta\)：模型参数  
- \(\eta\)：学习率  
- \(J(\theta)\)：损失函数  

目标  
找到使 \(J(\theta)\) 最小的最优参数  
\[
\theta^* = \arg\min_\theta J(\theta)
\]

---

## 11. 学习率（Learning Rate）

定义  
控制每次参数更新步幅大小的超参数。

影响  
- 过大：更新过度，损失震荡或发散  
- 过小：收敛缓慢，易陷入局部最优  

---

## 12. 训练集、验证集与测试集

用途  
- 训练集：模型学习与参数优化  
- 验证集：调整超参数、监控过拟合  
- 测试集：评估最终模型的泛化性能  

不要用训练集做最终评估，因为模型可能对训练数据产生过拟合。

---

## 13. 分类评估指标

- 准确率（Accuracy）：所有样本中预测正确的比例  
  \[
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
  \]
- 精确率（Precision）：预测为正类样本中真实正类的比例  
  \[
  \text{Precision} = \frac{TP}{TP + FP}
  \]
- 召回率（Recall）：真实正类样本中被正确预测为正类的比例  
  \[
  \text{Recall} = \frac{TP}{TP + FN}
  \]

---

## 14. 什么是神经网络？单层感知机示例

定义  
由大量带权重连接的“神经元”节点组成，通过加权求和和激活函数协同工作，逼近复杂函数映射。

单层感知机工作流程  
1. 输入向量 \(x\) 与权重矩阵 \(W\) 相乘  
2. 加上偏置 \(b\)  
3. 通过激活函数 \(f\) 得到输出：  
   \[
   \hat y = f(Wx + b)
   \]  

限制  
只能解决线性可分问题，复杂任务需多层网络。

---

写在最后：  
回答这些问题比想象中学到更多细节，依然很充实😁
